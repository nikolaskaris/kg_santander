{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import gc\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Fastai\n",
    "from fastai import *\n",
    "from fastai.tabular import *\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 3710\n",
    "np.random.seed(random_state)\n",
    "tr = pd.read_csv('/storage/santander_comp/train_freq.csv')\n",
    "te = pd.read_csv('/storage/santander_comp/test_freq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_small = tr.sample(frac=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"% of target in small train == 1: {:01.2f}%\".format(len(tr_small.loc[tr_small['target'] == 1]) / len(tr_small) * 100))\n",
    "print(\"% of target in train == 1: {:01.2f}%\".format(len(tr.loc[tr['target'] == 1]) / len(tr) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for feature in tr.columns if feature not in ['ID_code', 'target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(tr[features],\n",
    "                                                  tr['target'],\n",
    "                                                  test_size = 0.20,\n",
    "                                                  random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "# #     \"bagging_fraction\" : [0.3, 0.5],\n",
    "# #     \"bagging_freq\": [3, 5],\n",
    "# #     \"feature_fraction\" : [0.03, 0.04],\n",
    "#       \"learning_rate\" : [0.0005, 0.003, 0.007, 0.008, 0.01, 0.02],\n",
    "# #     \"min_data_in_leaf\": [5, 25, 60, 80, 100, 150],\n",
    "# #     \"num_leaves\" : [2, 20, 100, 1000],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n",
    "#           objective = 'binary',\n",
    "#           metric = 'auc',\n",
    "#           num_leaves = params['num_leaves'],\n",
    "#           num_threads = params['num_threads'],\n",
    "#           learning_rate = params['learning_rate'],               \n",
    "#           bagging_freq = params['bagging_freq'],\n",
    "#           bagging_fraction = params['bagging_fraction'],\n",
    "#           feature_fraction = params['feature_fraction'],\n",
    "#           min_data_in_leaf = params['min_data_in_leaf'],\n",
    "#           min_sum_hessian_in_leaf = params['min_sum_hessian_in_leaf'],\n",
    "#           tree_learner = params['tree_learner'],\n",
    "#           boost_from_average = params['boost_from_average'],\n",
    "# #           bagging_seed = params['bagging_seed'],\n",
    "#           verbosity = params['verbosity'],\n",
    "#           seed = params['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_idx = X_train.index\n",
    "val_idx = X_val.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\" : \"binary\",\n",
    "    \"metric\" : \"auc\",\n",
    "    \"boosting_type\": 'gbdt',\n",
    "    \"num_leaves\" : 13,\n",
    "    \"learning_rate\" : 0.0085,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_fraction\" : 0.4,\n",
    "    \"feature_fraction\" : 0.04,\n",
    "    \"min_data_in_leaf\": 80,\n",
    "    \"min_sum_hessian_in_leaf\": 10,\n",
    "    \"tree_learner\": \"serial\",\n",
    "    \"boost_from_average\": \"false\",\n",
    "    \"verbosity\" : -1,\n",
    "    \"seed\": random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = tr[['ID_code', 'target']]\n",
    "oof['predict'] = 0\n",
    "predictions = tr[['ID_code']]\n",
    "val_aucs = []\n",
    "feature_importance_df = pd.DataFrame()\n",
    "p_val, yp = 0, 0\n",
    "trn_data = lgb.Dataset(X_train, label = y_train)\n",
    "val_data = lgb.Dataset(X_val, label = y_val)\n",
    "evals_result = {}\n",
    "lgb_clf = lgb.train(params,\n",
    "                        trn_data,\n",
    "                        100000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        early_stopping_rounds=2000,\n",
    "                        verbose_eval = 1000,\n",
    "                        evals_result=evals_result\n",
    "                       )\n",
    "\n",
    "p_val += lgb_clf.predict(X_val)\n",
    "feature_importance_df[\"feature\"] = features\n",
    "feature_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
    "oof['predict'][val_idx] = p_val\n",
    "val_score = roc_auc_score(y_val, p_val)\n",
    "val_aucs.append(val_score)\n",
    "predictions = yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for feature in tr.columns if feature not in ['ID_code', 'target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(tr[features],\n",
    "                                                  tr['target'],\n",
    "                                                  test_size = 0.20,\n",
    "                                                  random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\" : \"binary\",\n",
    "    \"metric\" : \"auc\",\n",
    "    \"boosting_type\": 'gbdt',\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\" : 16,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_fraction\" : 0.333,\n",
    "    \"feature_fraction\" : 0.041,\n",
    "    \"min_data_in_leaf\": 140,\n",
    "    \"min_sum_hessian_in_leaf\": 10,\n",
    "    \"tree_learner\": \"serial\",\n",
    "    \"boost_from_average\": \"false\",\n",
    "    \"verbosity\" : -1,\n",
    "    \"seed\": random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 2000 rounds.\n",
      "[1000]\ttraining's auc: 0.908953\tvalid_1's auc: 0.885829\n",
      "[2000]\ttraining's auc: 0.920456\tvalid_1's auc: 0.893866\n",
      "[3000]\ttraining's auc: 0.928328\tvalid_1's auc: 0.897185\n",
      "[4000]\ttraining's auc: 0.934585\tvalid_1's auc: 0.899009\n",
      "[5000]\ttraining's auc: 0.940149\tvalid_1's auc: 0.900086\n",
      "[6000]\ttraining's auc: 0.945277\tvalid_1's auc: 0.900635\n",
      "[7000]\ttraining's auc: 0.950015\tvalid_1's auc: 0.900774\n",
      "[8000]\ttraining's auc: 0.954475\tvalid_1's auc: 0.900795\n",
      "[9000]\ttraining's auc: 0.958661\tvalid_1's auc: 0.900737\n",
      "[10000]\ttraining's auc: 0.962585\tvalid_1's auc: 0.900796\n",
      "Early stopping, best iteration is:\n",
      "[8398]\ttraining's auc: 0.956163\tvalid_1's auc: 0.900892\n"
     ]
    }
   ],
   "source": [
    "trn_data = lgb.Dataset(X_train, label = y_train)\n",
    "val_data = lgb.Dataset(X_val, label = y_val)\n",
    "evals_result = {}\n",
    "lgb_clf = lgb.train(params,\n",
    "                        trn_data,\n",
    "                        100000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        early_stopping_rounds=2000,\n",
    "                        verbose_eval = 1000,\n",
    "#                         learning_rates=lambda iter: 0.006 * (0.97 ** iter),\n",
    "                        evals_result=evals_result\n",
    "                       )\n",
    "\n",
    "\n",
    "feature_importance_df[\"feature\"] = features\n",
    "feature_importance_df[\"importance\"] = lgb_clf.feature_importance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
